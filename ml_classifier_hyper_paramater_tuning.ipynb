{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mneural_network\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MLPClassifier\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Loading in CSV's using Pandas then formatting for use in algorithms\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m steel_plates_fault \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/saxtonjosh/ML-Classifiers-Hyperparamater-Testing/steel-plates-fault_csv.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     19\u001b[0m ionosphere \u001b[38;5;241m=\u001b[39m load_data(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/saxtonjosh/ML-Classifiers-Hyperparamater-Testing/ionosphere_csv.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m banknotes \u001b[38;5;241m=\u001b[39m load_data(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/saxtonjosh/ML-Classifiers-Hyperparamater-Testing/steel-plates-fault_csv.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_data' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "from statistics import mean\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Loading in CSV's using Pandas then formatting for use in algorithms\n",
    "steel_plates_fault = load_data(\"https://raw.githubusercontent.com/saxtonjosh/ML-Classifiers-Hyperparamater-Testing/steel-plates-fault_csv.csv\")\n",
    "ionosphere = load_data(\"https://raw.githubusercontent.com/saxtonjosh/ML-Classifiers-Hyperparamater-Testing/ionosphere_csv.csv\")\n",
    "banknotes = load_data(\"https://raw.githubusercontent.com/saxtonjosh/ML-Classifiers-Hyperparamater-Testing/steel-plates-fault_csv.csv\")\n",
    "\n",
    "datasets = {\"steel_plates_fault\": steel_plates_fault,\n",
    "            \"ionosphere\": ionosphere,\n",
    "            \"banknotes\" : banknotes,\n",
    "           }\n",
    "\n",
    "classifiers = {\n",
    "        \"KNearestNeighbor\": KNeighborsClassifier,\n",
    "        \"NaiveBayes\": GaussianNB,\n",
    "        \"DecisionTree\": DecisionTreeClassifier,\n",
    "        \"LogisticRegression\": LogisticRegression,\n",
    "        \"GradientBoosting\": GradientBoostingClassifier,\n",
    "        \"RandomForest\": RandomForestClassifier,\n",
    "        \"NeuralNet\": MLPClassifier\n",
    "}\n",
    "\n",
    "clf_variables = {\n",
    "        \"n_neighbors\": list(range(1, 6)),\n",
    "        \"var_smoothing\": [1e-9, 1e-5, 1e-1],\n",
    "        \"max_depth\": [1, 3, 5, 8, 10],\n",
    "        \"C\": [.1, .5, 1.0, 2.0, 5.0],\n",
    "        \"max_depth\": [1, 3, 5, 8, 10],\n",
    "        \"max_depth\": [1, 3, 5, 8, 10],\n",
    "        \"alpha\": [1e-5, 1e-3, 0.1, 10.0]\n",
    "}\n",
    "\n",
    "clf_names = [\"KNearestNeighbor\", \"NaiveBayes\", \"DecisionTree\", \"LogisticRegression\", \"GradientBoosting\",\n",
    "         \"RandomForest\", \"NeuralNet\"\n",
    "        ]\n",
    "\n",
    "clf_var_names = [\"n_neighbors\", \"var_smoothing\", \"max_depth\", \"C\", \"max_depth\", \"max_depth\", \"alpha\"]\n",
    "\n",
    "# Helper method for loading data\n",
    "def load_data(url):\n",
    "    df = pandas.read_csv(url)\n",
    "    data_array = df.to_numpy()\n",
    "    X = data_array[:, :-1] # for all but last column\n",
    "    y = data_array[:, -1] # for last column\n",
    "    dataset = (X, y)\n",
    "    return dataset\n",
    "\n",
    "# Function to dynamically create a classifier instance\n",
    "def get_clf(name, index):\n",
    "    if name not in clf_names:\n",
    "        raise ValueError(f\"{name} is not a recognised option\")\n",
    "    classifier = classifiers[name]\n",
    "    if (name == \"NeuralNet\"):\n",
    "        return classifier(max_iter = 800, random_state = 4)\n",
    "    if (index > 1):\n",
    "        return classifier(random_state = 4)\n",
    "    return classifier()\n",
    "\n",
    "# where the train test splits will be stored\n",
    "X_trains = []\n",
    "X_tests = []\n",
    "y_trains = []\n",
    "y_tests = []\n",
    "\n",
    "# preprocess each dataset, split into 50 different training and test sets\n",
    "for name, ds in datasets.items():\n",
    "    X, y = ds\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    X_train = [0] * 50\n",
    "    X_test = [0] * 50\n",
    "    y_train = [0] * 50\n",
    "    y_test = [0] * 50\n",
    "    \n",
    "    # split dataset into 50 different train/test splits\n",
    "    i = 0\n",
    "    for i in range(50):\n",
    "        X_train[i], X_test[i], y_train[i], y_test[i] = train_test_split(X, y, test_size = 0.5, random_state = i)\n",
    "        i += 1\n",
    "    \n",
    "    X_trains.append(X_train)\n",
    "    X_tests.append(X_test)\n",
    "    y_trains.append(y_train)\n",
    "    y_tests.append(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accs = np.zeros((7,3))\n",
    "best_values = np.zeros((7,3))\n",
    "\n",
    "# iterate over each classifier\n",
    "for clf_count, (clf_name, var_name) in enumerate(zip(clf_names, clf_var_names)):\n",
    "    clf = get_clf(clf_name, clf_count)\n",
    "    params = clf.get_params()\n",
    "    fig, axs = plt.subplots(1, 3, figsize = (15, 5))\n",
    "    \n",
    "    # iterate over each dataset\n",
    "    for ds_count, ds_name in enumerate(datasets.keys()):\n",
    "        var_values = clf_variables[var_name]\n",
    "        ds_accs = []\n",
    "        \n",
    "        # change associated hypervariable value per test\n",
    "        for var_value in var_values:\n",
    "            params[var_name] = var_value\n",
    "            clf.set_params(**params)\n",
    "            val_accs = []\n",
    "\n",
    "            # repeat classifier fit and evaluate accuracy for each train/test split\n",
    "            j = 0\n",
    "            for j in range(50):\n",
    "                clf.fit(X_trains[ds_count][j], (y_trains[ds_count])[j])\n",
    "                val_accs.append(clf.score(X_tests[ds_count][j], y_tests[ds_count][j]))\n",
    "                j += 1\n",
    "            \n",
    "            # find the best mean accuracy for each classifer-dataset pairing\n",
    "            avg = mean(val_accs)\n",
    "            if (avg >= best_accs[clf_count, ds_count]):\n",
    "                best_accs[clf_count, ds_count] = avg\n",
    "                best_values[clf_count, ds_count] = var_value\n",
    "            ds_accs.append(val_accs)\n",
    "            \n",
    "        # draw the boxplots\n",
    "        axs[ds_count].boxplot(ds_accs)\n",
    "        axs[ds_count].set_xticklabels(var_values)\n",
    "        axs[ds_count].set_title(ds_name)\n",
    "        axs[ds_count].set_xlabel(var_name)\n",
    "        if (ds_count == 0):\n",
    "            axs[ds_count].set_ylabel(clf_name)\n",
    "\n",
    "        ds_count += 1\n",
    "        \n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    clf_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Best mean accuracies from the 50 different train-test splits for each of the dataset-classifier pairs\\n\")\n",
    "print(pandas.DataFrame(best_accs, clf_names, datasets.keys()))\n",
    "print(\"Best hyperparameter value based on average accuracy across all train/test splits\\n\")\n",
    "print(pandas.DataFrame(best_values, clf_names, datasets.keys()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
